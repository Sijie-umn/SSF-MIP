{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute KNN similarities\n",
    "\n",
    "Computes similarities between each pair of dates based on how skillfully the history of one date predicts the history of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package loading\n",
    "\n",
    "# Autoreload packages that are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Plotting magic\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4\n",
    "import time\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"experiments\":\n",
    "    os.chdir(os.path.join(\"..\",\"..\"))\n",
    "\n",
    "# Adds 'experiments' folder to path to load experiments_util\n",
    "sys.path.insert(0, 'src/experiments')\n",
    "# Load general utility functions\n",
    "from experiments_util import *\n",
    "# Load functionality for fitting and predicting\n",
    "from fit_and_predict import *\n",
    "# Load functionality for evaluation\n",
    "from skill import *\n",
    "\n",
    "## Prepare experimental results directory structure\n",
    "\n",
    "# Set hindcast_year to None to obtain forecasts and to a specific year to obtain hindcasts\n",
    "hindcast_year = None\n",
    "\n",
    "# Choose the name of this experiment\n",
    "experiment = \"knn\"\n",
    "if hindcast_year is not None:\n",
    "    experiment = \"knn-hindcast_{}\".format(hindcast_year) ### For hindcasts\n",
    "    \n",
    "# Name of cache directory for storing non-submission-date specific\n",
    "# intermediate files\n",
    "cache_dir = os.path.join('knn_mip')\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_id = \"contest_tmp2m\"\n",
    "gt_col= \"tmp2m\"\n",
    "anom_col= \"tmp2m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ground truth cosine similarities between pairs of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment == \"knn\":\n",
    "    anoms=pd.read_hdf('data/tmp2m_western_us_anom_rmm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms.reset_index(['lat','lon','start_date'],inplace=True)\n",
    "anoms=anoms=anoms[anoms.start_date>='1990-01-01']\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(2004,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(1984,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(1988,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(1992,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(1996,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(2000,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(2004,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(2008,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(2012,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(2016,2,29)]\n",
    "anoms=anoms[anoms.start_date!=pd.Timestamp(2020,2,29)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataframe to have one row per start date and one column per (lat,lon)\n",
    "tic(); anoms = anoms.set_index(['lat','lon','start_date']).unstack(['lat','lon']); toc()\n",
    "# Drop start dates that have no measurements (e.g., leap days, which have no climatology)\n",
    "anoms = anoms.dropna(axis='index', how='all')\n",
    "# Normalize each start_date's measurements by its Euclidean norm\n",
    "tic()\n",
    "norms = np.sqrt(np.square(anoms).sum(axis=1))\n",
    "anoms = anoms.divide(norms, axis=0)\n",
    "toc()\n",
    "# Compute the cosine similarity between each pair of dates by computing all inner products\n",
    "tic(); gt_cosines = anoms.dot(anoms.transpose()); toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_cosines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each date is represented by its past_days most recent observed measurements (i.e., \n",
    "# the past_days most recent measurements at least start_delta days before the date).\n",
    "# The similarity of two dates is the average cosine similarity their past_days\n",
    "# associated measurements.\n",
    "\n",
    "# The number of past days that should contribute to measure of similarity\n",
    "past_days = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity measure between pairs of target dates assuming start_delta = 0\n",
    "That is, assuming that we have access to the ground truth measurement with start date equal to the target date.\n",
    "Later we will shift by start_delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if base similarities have been computed previously\n",
    "regen_similarities0 = True\n",
    "similarities0_file = os.path.join(\n",
    "    'knn_mip/similarities0-{}-days{}.h5'.format(gt_id,past_days))\n",
    "if regen_similarities0 or not os.path.isfile(similarities0_file):\n",
    "    # Initially incorporate unshifted cosine similarities \n",
    "    # (representing the cosine similarity of the first past day)\n",
    "    tic()\n",
    "    similarities0 = gt_cosines.copy()\n",
    "    toc()\n",
    "\n",
    "    # Now, for each remaining past day, sum over additionally shifted measurements\n",
    "    # NOTE: this has the effect of ignoring (i.e., skipping over) dates that don't \n",
    "    # exist in gt_cosines like leap days\n",
    "    tic()\n",
    "    for m in range(1,past_days):\n",
    "    #for m in range(1,2):\n",
    "        similarities0 += gt_cosines.shift(m, axis='rows').shift(m, axis='columns')\n",
    "        sys.stdout.write(str(m)+' ')\n",
    "    toc()\n",
    "\n",
    "    # Normalize similarities by number of past days\n",
    "    similarities0 /= past_days\n",
    "    # Write similarities0 to file\n",
    "    print \"Saving similarities0 to \"+similarities0_file; tic()\n",
    "    similarities0.to_hdf(similarities0_file, key=\"data\", mode=\"w\"); toc()\n",
    "else:\n",
    "    # Read base similarities from disk\n",
    "    print \"Reading similarities0 from \"+similarities0_file; tic()\n",
    "    similarities0 = pd.read_hdf(similarities0_file); toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction horizon\n",
    "target_horizon = \"34w\" # \"34w\" or \"56w\"\n",
    "\n",
    "# Only use measurements available this many days prior to \n",
    "# official contest submission date\n",
    "days_early = 365 - (14 + get_forecast_delta(target_horizon, days_early = 0)) \n",
    "\n",
    "## Process inputs\n",
    "\n",
    "# Number of days between start date of most recently observed measurement\n",
    "# (2 weeks to observe complete measurement) and start date of target period \n",
    "# (2 or 4 weeks plus days early days ahead)\n",
    "aggregation_days = 14\n",
    "start_delta = (aggregation_days + \n",
    "               get_forecast_delta(target_horizon, days_early = days_early))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift similarities by start_delta\n",
    "The rows and columns of similarities represent target dates, and the similarities are now based on ground truth measurements from start_delta days prior to each target date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The earliest measurement available is from start_delta days prior to target day, \n",
    "# so shift rows and columns of similarities by start_delta and extend index accordingly\n",
    "# NOTE: For some reason, shifting columns doesn't extend column index, so I'm transposing and shifting\n",
    "# rows\n",
    "tic()\n",
    "similarities = similarities0.shift(start_delta, axis='rows', freq='D').transpose().shift(start_delta, axis='rows', freq='D')\n",
    "toc()\n",
    "# Index extension has the side effect of creating leap days (e.g., 2012-02-29) and removing \n",
    "# the date start_delta days later (e.g., datetime.date(2012,2,29) + timedelta(start_delta))\n",
    "# Add one day to each date in the range [datetime.date(2012,2,29), \n",
    "# datetime.date(2012,2,29) + timedelta(start_delta)) to remove leap days\n",
    "def fix_date(date):\n",
    "    if date.is_leap_year:\n",
    "        # Identify the affected dates in this current date's year\n",
    "        affected_dates = pd.date_range('{}-02-29'.format(date.year), periods=start_delta, freq='D')\n",
    "    elif date.replace(year=date.year-1).is_leap_year:\n",
    "        # Identify the affected dates starting from prior year\n",
    "        affected_dates = pd.date_range('{}-02-29'.format(date.year-1), periods=start_delta, freq='D')\n",
    "    else:\n",
    "        # Only modify leap year dates and dates following leap year\n",
    "        return date\n",
    "    # Shift date by 1 day if affected\n",
    "    return date + timedelta(1) if date in affected_dates else date\n",
    "tic()\n",
    "new_index = [fix_date(date) for date in similarities.index]\n",
    "toc()\n",
    "tic()\n",
    "similarities = similarities.reindex(new_index)\n",
    "similarities.columns = new_index\n",
    "toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict similarities to viable neighbors\n",
    "Viable neighbors are those with available ground truth data (as evidenced by anoms or gt_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if viable similarities have been computed previously\n",
    "regen_viable_similarities = True\n",
    "viable_similarities_file = os.path.join(\n",
    "    cache_dir,'viable_similarities-{}-{}-days{}-early{}.h5'.format(gt_id,target_horizon,past_days,days_early))\n",
    "if regen_viable_similarities or not os.path.isfile(viable_similarities_file):\n",
    "    viable_similarities = similarities[similarities.index.isin(gt_cosines.index)]\n",
    "    print \"Saving viable_similarities to \"+viable_similarities_file; tic()\n",
    "    viable_similarities.to_hdf(viable_similarities_file, key=\"data\", mode=\"w\"); toc()\n",
    "else:\n",
    "    # Read viable similarities from disk\n",
    "    print \"Reading viable similarities from \"+viable_similarities_file; tic()\n",
    "    viable_similarities = pd.read_hdf(viable_similarities_file); toc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:s2s] *",
   "language": "python",
   "name": "conda-env-s2s-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
